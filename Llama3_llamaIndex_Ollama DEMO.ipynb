{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install necessary packages\n",
    "\n",
    "Run the following commands to install the necessary packages:\n",
    "\n",
    "Before downloading: <u><b>Download Llama3 with Ollama</b></u>\n",
    "\n",
    "```bash\n",
    "!pip install llama-index==0.10.32  # Version: 0.10.32\n",
    "!pip install llama-index-core==0.10.32  # Version: 0.10.32\n",
    "!pip install llama-index-llms-ollama==0.1.2  # Version: 0.1.2\n",
    "!pip install llama-index-readers-web==0.1.10  # Version: 0.1.10\n",
    "!pip install llama-index-embeddings-ollama==0.1.2  # Version: 0.1.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama Index Documentation\n",
    "\n",
    "Here are some useful links to get started with Llama Index:\n",
    "\n",
    "1. [Starter Example Local](https://docs.llamaindex.ai/en/stable/getting_started/starter_example_local/)\n",
    "2. [Data Connectors - Web Page Demo](https://docs.llamaindex.ai/en/stable/examples/data_connectors/WebPageDemo/)\n",
    "3. [Embeddings - Ollama Embedding](https://docs.llamaindex.ai/en/stable/examples/embeddings/ollama_embedding/)\n",
    "4. [Model - Llama3 Download with Ollama ](https://ollama.com/library/llama3)\n",
    "\n",
    "\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Load data and build an index - <span style=\"color:LightSkyBlue\">WEB</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "\n",
    "documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
    "    [\"https://zeeshankhawar.medium.com/connecting-chatgpt-with-your-own-data-using-llama-index-and-langchain-74ba79fb7429#:~:text=LangChain%20vs%20LlamaIndex&text=You%20can%20use%20data%20loaders,ability%20to%20create%20hierarchical%20indexes.\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create <u><b>Llama3</b></u> model with <span style=\"color:MediumOrchid\">LlamaIndex</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex,  Settings\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.llms.ollama import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ollama_embedding = OllamaEmbedding(\n",
    "    model_name=\"llama3\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    ollama_additional_kwargs={\"mirostat\": 0},\n",
    ")\n",
    "Settings.embed_model = ollama_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = Ollama(model=\"llama3\", request_timeout=30.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Query your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm happy to help! However, I must point out that there seems to be a discrepancy in the provided context information. The text appears to be asking me to enable JavaScript and cookies to continue, but it doesn't seem to have any relevance to the query \"Benefits of llama Index to langchain\".\n",
      "\n",
      "Assuming that the query is somehow related to the context, I'll take a wild guess and say that the benefits of llama Index to langchain might include improved search results, enhanced natural language processing capabilities, or increased efficiency in processing large volumes of data.\n",
      "\n",
      "Please let me know if this answer aligns with your expectations.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"Benefits of llama Index to langchain\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
