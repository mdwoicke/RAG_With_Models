{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# List of Packages to Download\n",
    "\n",
    "```python\n",
    "!pip install langchain-chroma\n",
    "!pip install langchain\n",
    "!pip install langchain_community\n",
    "!pip install langchainhub\n",
    "!pip install -qU langchain-openai\n",
    "\n",
    "```\n",
    "## Also fix api key\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"\n",
    "```\n",
    "\n",
    "\n",
    "______________________________________________________________________________\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Get a Data Loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='GPT4 Omni — So much more than just a voice assistant | by katerinaptrv | May, 2024 | MediumOpen in appSign upSign inWriteSign upSign inGPT4 Omni — So much more than just a voice assistantkaterinaptrv·Follow3 min read·1 hour ago--ListenShareToday we have the OpenAI Spring announcement and it was mind blowing, I guess we all can agree here. I was playing with my new voice assistant for most of this night(mentions of the movie HER very accurate in this scenario).But as revolutionary, incredible and world changing as the voice capability is, the whole GPT4o model goes so much further than that.I must confess, the voice thing got me for real, so even I took a time to really stop and read the technical announcement of this model and when I finally did just now my mind was blown again.I was thinking GPT4o was just a better optimized version of GPT-4 Turbo, this time with better reasoning, less latency and trained for voice conversations. And that they basically just combined all the tech they already had with Whisper and TTS along with calls with the new optimized model and integrated all in ChatGPT in a very effective way.But after reading the technical report of the model, i saw this:With GPT-4o, we trained a single new model end-to-end across text, vision, and audio, meaning that all inputs and outputs are processed by the same neural network. Because GPT-4o is our first model combining all of these modalities, we are still just scratching the surface of exploring what the model can do and its limitations.A single new model with end to end text, audio and vision multimodality!!A single model that can have an input of text/audio/image and make an output of text/audio/image.I know I posted at the end of the last that the future of GenAI was multimodality across all modalities and we see initiatives in that sense.But I never thought that this future would be in May of 2024 and we had a model that is able to process and generate all major modalities while still having a good response time.Now, this is beyond revolutionary, OpenAI cleans the board again, no one has anything close to this and the possibilities of such a model are so big that our minds have trouble processing it.We have to revise all our previous concepts and ideas because limitations that make it not reality before might not exist today and also prepare our minds for new renovated ideas of solutions we never even imagined possible before.PS1: They did not release access to all modalities in their api yet, right now we have text and image. So, we can think about solutions with the others but have to wait for their release that has no defined date until now.PS2: By the examples the model also generates 3d images.PS3: Is valid to mention that the model today cost half the price of GPT-4 Turbo, so it is way more than Turbo ever was and is more efficient costing less.Here is some benchmarks showing some on par performance with most recent top models:You can see more about this model here, including examples of use:Hello GPT-4o | OpenAIArtificial IntelligenceAITechnologyMachine LearningDeep Learning----FollowWritten by katerinaptrv170 FollowersFollowHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://medium.com/@daniellefranca96/gpt4-omni-so-much-more-than-just-a-voice-assistant-c5ae43bdc6be', 'title': 'GPT4 Omni — So much more than just a voice assistant | by katerinaptrv | May, 2024 | Medium', 'description': 'Today we have the OpenAI Spring announcement and it was mind blowing, I guess we all can agree here. I was playing with my new voice assistant for most of this night(mentions of the movie HER very…', 'language': 'en'})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = WebBaseLoader(\"https://medium.com/@daniellefranca96/gpt4-omni-so-much-more-than-just-a-voice-assistant-c5ae43bdc6be\")\n",
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Convert data to Vector Database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x14c171fd0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Chroma.from_documents(documents=data, embedding=OpenAIEmbeddings(model=\"text-embedding-3-large\"), persist_directory=\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Make a RAG pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "vectorstore = Chroma(persist_directory=\"./chroma_db\", embedding_function=OpenAIEmbeddings(model=\"text-embedding-3-large\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm,\n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        chain_type_kwargs={\"prompt\": prompt}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    }
   ],
   "source": [
    "question = \"Make SEO optimized GPT4o youtube description\"\n",
    "result = qa_chain({\"query\": question })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Explore the revolutionary capabilities of GPT4 Omni in our latest video! Discover how this groundbreaking AI model integrates text, vision, and audio processing to deliver unparalleled performance and efficiency. Subscribe now to stay updated on the future of AI technology! #GPT4Omni #AI #MachineLearning #DeepLearning #OpenAI'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Explore the revolutionary capabilities of GPT4 Omni in our latest video! '\n",
      " 'Discover how this groundbreaking AI model integrates text, vision, and audio '\n",
      " 'processing to deliver unparalleled performance and efficiency. Subscribe now '\n",
      " 'to stay updated on the future of AI technology! #GPT4Omni #AI '\n",
      " '#MachineLearning #DeepLearning #OpenAI')\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
